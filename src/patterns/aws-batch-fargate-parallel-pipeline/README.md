# Batch Fargate Parallel Pipeline

## Description
This construct provides a way to process objects stored in S3 with a provided container process using a Batch Job Queue and a ECS Fargate Compute Environment.

The Batch Fargate Parallel Pipeline pattern creates a Step Function Chain, and optionally a State Machine, with a [Map State](https://docs.aws.amazon.com/step-functions/latest/dg/state-map.html) that iterates over item keys. The item keys are generated by the [Sfn Map State Input Generator Lambda Function](../../constructs/core/lambda/sfnMapStateInputGenerator/index.py). Inside the Map State, a Batch Submit Job Task is created for each item, which is executed on a Fargate task.

## Usage
Import and deploy the construct in a stack. 

### Props

* `pipelineName`: {string} The name of the pipeline.
* `batchFargateSubmitJobSfnChainConstructProps`: {BatchFargateSubmitJobSfnChainConstructProps} [The Batch Fargate Submit Job Sfn Chain Construct](../../constructs/aws-batch-fargate-submit-job-sfn-chain/index.ts) Props.
* `createStateMachine`: {boolean} used to set whether a [State Machine with Log Group](../../constructs/aws-state-machine-with-log-group-from-chain/index.ts) is created.
* `stateMachineTimeout`: {Duration} The time after which the state machine will terminate.

Example Props:
```
{
    pipelineName: 'BlenderBoundingBoxMeshes',
    batchFargateSubmitJobSfnChainConstructProps: {
        stepConfig: new StepConfig(StepType.BOUNDINGBOX, StepSchema.INPUT_SINGLE_FILE_OUTPUT_PREFIX),
        bucket: sourceAssetBucket,
        constructJobDefinitionFunction: constructJobDefinitionFunction,
        batchEcsJobDefinition: batchEcsJobDefinition,
        batchFargateConstruct: batchFargateConstruct
    },
    createStateMachine: true,
    stateMachineTimeout: Duration.hours(6)
}
```

### State Machine Execution

If creating a Step Function State Machine, initiate execution with input state of the following format:
```
{
    "state_machine_global_data": {
        "job_name": <name_of_job>
    },
    "step_data": { 
        "type": <type_of_job>
        "source_bucket": <bucket_name>
        "inputs_prefix": <source_input_bucket_prefix>
        "outputs_prefix": <output_bucket_prefix>
    }
}
```

where:

* `state_machine_global_data` persists through the pipeline
* `step_data` is passed to the next state, but may not persist through the entire pipeline.
* `job_name` is the name of the current job
* `type` is the type identifier for the job. This is a string value and is entirely dependent on container business logic interpretation, e.g. "JOIN".
* `source_bucket` is the bucket name of the source objects to be processed, e.g. "my-bucket-name".
* `inputs_prefix` is the S3 prefix pointing to the folder location of the input source objects, e.g. "my-project/my-job/source".
* `outputs_prefix` is the S3 prefix pointing to the folder location where the computed output objects should be stored, e.g. "my-project/my-job/output".

Example State Machine input state:
```
{
  "state_machine_global_data": {
    "job_name": "test-map"
  },
  "step_data": {
    "type": "BOUNDINGBOX",
    "source_bucket": "blenderboundingboxmeshess-sourceassetbucketXXXXXXXX",
    "inputs_prefix": "input",
    "outputs_prefix": "bounding-box"
  }
}
```

If creating a Step Function Chain only (`createStateMachine` = false), ensure that the state is in the above format before being consumed by this chain.
